


## Paradigmes de conception

Le conception de n'importe quel dispositif numérique nécessite une équipe de développement. À minima, cette équipe sera constituée d'un ou plusieurs développeurs. Au fil du temps, la tendance s'est affirmer de rendre les équipes transdisciplinaire et de faire travailler ensemble différents corps de métiers. Une équipe de développement peut donc faire appel à des ergonomes, des ethnologues, des ethnométhodologues, des designers, etc. Un schéma courant dans les équipes consiste à ce qu'un design d'interface produise le graphisme de l'interface et les interactions en coordination avec un développeur chargé d'intégrer dans le système ces interfaces. Comme dans toute équipe, un fonctionnement en silos et un manque de communication peuvent être dramatiques pour la qualités du produit fini. Par certains aspects, les paradigmes, les méthodes et les enjeux peuvent se concurrencer au sein d'une équipe, voir s'opposer. Enfin, il me semble qu'on ne peut faire abstraction d'un milieu économique et de ses acteurs, qui influencent les tendances et les idéologies de développement des logiciels. 
{: .abstract}



### Opposition design et développement

Issus d'un cursus en design, j'ai été confronté à une vision particulière du métier de développeur. Le statut de développeur génère souvent des railleries et moquerie au sein de la communauté du design. Ces plaisanteries bon enfant, si elles ne sont jamais vraiment des critiques, révèlent que le designer considère le développeur d'une manière particulière et inversement. Dans cette partie, l'essentiel de de l'analyse sur le regard croisé designer/développeur se basera sur mon expérience personnelle, mais aussi sur l'analyse de documents issus d'internet. Au delà de ces considérations, il me faudra ensuite questionner les méthodes de conception, qui sont aussi porteuses de sens. 
{: .abstract}



#### Retour d'expérience

Mon apprentissage du design a été un parcours qui a pu semblé long et complexe. Le premier contact avec le mot « design » lui-même remonte à mon cursus en BAC Sciences et technologies industrielles. Derrière cette appellation se cache une initiation, qui démarre en classe de seconde jusqu'au baccalauréat, à la conception d'objets, d'espaces, d'images, de sites web, etc. Je parle d'une initiation car je crois bien que l'essentiel de cette formation permet simplement, en tout cas pour ma part, à comprendre les champs différents du design, leurs enjeux et leurs codes, pour pouvoir s'orienter ensuite dans une spécialisation. Je vais décrire tout au long de ce cursus la relation aux outils techniques et aux compétences techniques car ce rapport me semble primordial pour expliquer un clivage qui apparaît plus loin entre designers et développeurs. L'accès aux logiciels de design en STI n'est pas critique : les rendus des projets se font sous la forme de planches papiers au format A3 manuscrites qui détaillent les propositions faites pour répondre au projet. Les compétences misent en œuvre dans la communication des projets relèvent donc d'une habileté au dessin et de la maîtrise d'une écriture manuscrite propre et lisible.

Le bac est suivis d'un BTS qui apporte les compétence techniques et recentre les réflexions sur les contraintes d'un domaine de production. Dans mon cas il s'agissait de la communication et du graphisme avec une option multimédia. À ce stade, les projets étaient rendus de manière mixte, numérisation/PAO, ce qui nécessitait l'apprentissage des logiciels utilisés dans les métiers du design. C'est aussi à ce stade que j'ai commencé la programmation de site web. Le besoin avait émergé d'une profonde frustration dans le processus de travail : nous étions capable de raisonner un graphisme, à partir de bases théoriques et considérations sémantiques qui permettaient de nous justifier, nous étions capable de créer une image valable dans un contexte de production, en maîtrisant les logiciels et en exportant les bons formats, mais nous étions incapable d'intégrer la moindre maquette graphique de site web dans une page. Les réponses aux projets n'étaient qu'un argumentaire autour de l'idée mais jamais une production finies. Ne vous méprenez pas : il est absolument essentiel de légitimer une idée, et cette communication est indispensable. Néanmoins, comme il serait frustrant pour un fabricant de jouer de ne jamais produire ses créations, il était pour moi frustrant de ne jamais voir fonctionner les choses. Je ne voulais pas pour autant devenir développeur, mais le développement et le prototypage étaient des conclusions et préambules logiques à ma pratique. Au sein du village du BTS, cette caractéristique m’apparaît avec du recul comme une bizarrerie. Très vite, je me suis retrouvé affilié au village des *geeks*. Un *geek* en soit est un individu très investis dans un domaine particulier souvent sous représentés et apparaissant comme en marge. Le *geek* manie justement un jargon très spécifique à son domaine de prédilection, ce qui le marginalise encore plus. L'informatique parmi d'autres domaines fait partie de ces domaines. Une blague récurrente à la fin de ce BTS était que « j'allais le faire en flash », en parlant de la réalisation des projets. L'expression elle même constituait une blague car je pestais assez souvent contre le flash, une technologie qui était déjà sur le déclin et qui a aujourd'hui quitté nos navigateurs. Cette marginalisation, dans un village dans lequel je m'estimais tout de même membre, est amoindris par un statut double designer/développeur. Il est donc fort à parier qu'un développeur, identifié en tant que tel dans un village de graphistes, ne profiterais pas de cet amoindrissement. C'est aussi durant ce BTS que j'ai pu m'acculturer à la sous-culture d'internet et à son partage massif d'images, qui a certainement renforcé certaines perceptions quand au rôles d'un graphiste et d'un développeur[^annexe_meme].

Les deux années suivantes en DSAA Design Interactif furent encore différentes. Mon envie de toujours « développer des trucs » en marge des projets de classe me caractérisait, mais le positionnement du DSAA sur l'usage prédominants de méthodes transdisciplinaires et sur la pratique d'un design global, c'est à dire à la croisée du graphisme, de l'objet, de l'interaction et de l'espace, amenait des profils très différents et spécialisés. Qui plus est la classe était scindée en deux groupes issus de deux corps d'apprentissage différents : le premier en design d'objet et le second en design graphique et multimédia. Le résultat qui en découle est que les pratiques et méthodes de chacun ne sont pas vraiment misent en marge par les autres, l'ensemble étant constitué du mélange hétéroclites des pratiques marginales de chacun. Bien sur, « on ne peut faire l'économie d'être vivant » comme le disait R. Jaulin à propos de la place de l'observateur dans l'observation , et l'on ne peut donc éviter de typifier, au sens d'A. Schütz (Schutz 1970), les autres. C'est ainsi que j'identifierais aujourd'hui certains types sociaux, inhérents à la sphère du design : designer graphique, designer d'interface, designer d'interaction, développeurs, designer d'expériences utilisateur, designer social, designer à tendance écologique, designer produit, designer industriel, designer artiste, monteur, *motion designer*, designer sonore, typographe, maquettiste, etc. Toutes ces personnes aux profils si variés, rassemblés en un même village, entretenaient donc une attitude curieuse et respectueuse des pratiques des autres, sans manquer toutefois d'exprimer l'amusement ou l'étonnement ou la critique que peuvent susciter des accounts étrangers.

Le décalage entre développeurs et designers tiens donc essentiellement à la réductions des méthodes et compétences qu'ils mettent en jeu dans leur pratique et à la valeur que donne le type A à la compétence du type B. Aussi, le langage et les accounts particuliers du village A et du village B participent au sentiment d'étrangeté réciproque qui peut apparaître. Pour bien comprendre les différences de paradigmes qui peuvent intervenir dans le travail du designer d'une part et du développeur d'autre part, il est aussi intéressant de se pencher sur les méthodes de travail et les représentations qui sont plébiscitée dans un domaine et dans l'autre.

[^annexe_meme]: Voir annexe « Le développeur et le designer dans la sous-culture internet »

<!--
TODO : Préciser que Jaulin n'a jamais écrit la formule
-->


#### Enjeux des modes de représentation, approche systémique

Le développement informatique est une affaire de moyens et de faisabilité. Les modes de représentation associés vont donc avoir pour but de cadrer les modalités dans lesquelles s'exprime l'artefact numérique. Ils permettent de rendre compte des interactions possibles et des besoins matériels et structurels qui en découlent. Nous allons nous intéresser à l'UML et aux différents types de diagrammes qu'il permet de produire. Il est important de noter que les diagrammes UMl sont conçus dans l'optique de définir le comportement d'un système lors de sa conception, il ne s'agit pas d'une définition du terrain initial, ni même d'acteurs en situation avant l'application de la solution. Ces diagrammes ne sont pas une aide à la conception mais bien un guide de production normalisé. Le diagramme des cas d'utilisation par exemple, qui définie les interactions entre des acteurs et le système, n'est pas un diagramme d'étude préalable, démontrant les actions existantes à prendre en compte dans la conception, mais une représentation des actions disponibles *in fine* dans l'application. Ces représentations sont des spécifications techniques qui permettent de passer à une phase de développement.

Comme tout descriptif technique et matériel, la justesse de cette représentation et la concordance avec le besoin de l'utilisateur dépend de l'étude préalable qui a permis de dégager les besoins sur le terrain. Néanmoins, on peut faire l'hypothèse qu'à l'imagine des couches d'abstractions, le langage formel UML induit une perte d'informations entre l'expression du besoin recueillie sur le terrain et sa représentation. Aussi, on peut considérer que la réification de l'utilisateur en acteur dans les schémas UML est déjà un arraisonnement de l'utilisateur au système, c'est à dire une considération de l'action de l'utilisateur comme dépendant du système qui lui propose l'action, et non l'inverse. B. Bachimont démontre dans son ouvrage (Bachimont 2010) que le dispositif arraisonne l'utilisateur à son fonctionnement, en faisant un opérateur plus qu'un utilisateur qui dispose d'un outil, un agent uniquement délégué à initier une action dans le système et à en traiter les réponses. Un logiciel n'est pas un outil dans la mesure où l'outil est décontextualisé et peut s'appliquer à différentes situations, à l'image d'un marteau par exemple. M. Baudouin-Lafon et O. Beaudoux dans un article ont d'ailleurs démontrés dans un article (Beaudoux et Beaudouin-Lafon 2001) que les applications et logiciels étaient conçus comme des environnements dans lesquels les actions sont préprogrammées et non transposables à d'autres environnements applicatifs. L'UML est une des étapes de la grammatisation de B. Stiegler vue précédemment : le langage formelle discrétise le réel et le réifie en objets numériques. Étant donné que la contrainte est en définitif une contrainte de production logicielle, on peut se demander dans quelle mesure l'UML n'est pas seulement l'instrument d'une « conformisation » aux contraintes techniques du système, plutôt qu'une extraction du réel contexte du terrain et de ses besoins. Un détail frappant concernant les diagramme des cas d'utilisation est la représentation des utilisateurs-acteurs-opérateurs dans le système. En bon langage formel, un unique pictogramme sert de représentation, la plupart du temps un « bonhomme bâton », associé à son rôle dans le système. Un article de blog de G. Dorne montre les différentes manières de dessiner l'être humain qu'utilisent 14 architectes différents (Dorne 2016). Chacun a une manière bien à lui de le représenter et on peut faire l'hypothèse que chacune, d'une manière réflexive dans le projet, véhicule et concourt à une idéologie globale du projet, en amont et pendant son développement. Dans le cas de l'UML qui est normalisé, on a affaire à une instance quelconque d'un acteur, sans trajectoire et sans contexte, de sorte qu'une fois installé dans le contexte final d'application, le logiciel rencontre souvent ses vraies problématiques. Définir l'utilisateur dans un projet et le représenter pour garder constamment à l'esprit à qui est destiné le logiciel est donc essentiel quel que soit la représentation. Tout processus qui vise à le normaliser nécessite une attention particulière pour ne pas qu'il soit écrasé par le système conçut initialement pour lui.

Une expression que j'ai pu entendre durant mon apprentissage du design, de la part de professeurs, et qui vaut autant en développant et en design est que « l'utilisateur est un idiot », sous-entendu qu'il faut considérer l'utilisateur final comme un idiot et simplifier les choses au maximum en nivelant la complexité des interactions et de l'interface par le bas, de sorte que « n'importe qui » puisse utiliser le logiciel. Pourtant, nous avons et aurons toujours besoin d'assistance technique dans les entreprises. Cela veut-il dire que l'utilisateur est effectivement idiot et qu'il faut pousser encore plus loin la formule ? Le problème malheureusement est souvent toujours le même : l'utilisateur conçu comme assujetti au système n'est pas pris en compte dans sa trajectoire personnelle. Les modèles et affordances qu'il peut calquer sur les interfaces ne sont pas prises en compte et il est considérer comme un idiot au titre des raisonnements qu'il préétablira sur le système et au titre de son ignorance du fonctionnement interne de l'artefact. Autrement dit, généraliser des méthodes à des utilisateurs non ciblés et non contextualisés, c'est utiliser des modèles d'idiots systémiques. L'idiot systémique, en miroir à la notion d'idiot culturel de H. Garfinkel (Garfinkel 1984) est un utilisateur dont les potentiels d'actions sont strictement bornés aux actions rendus disponibles par le programme. L'utilisateur n'est pas un idiot initialement, mais le système le fait devenir idiot. Cette hypothèse peut être appuyer par l'approche systémique de l'école de Palo Alto si l'on considère le logiciel comme un système homéostatique, préservant l'équilibre de leur état initial par des mécanismes de régulation (Picard et Marc 2013). Un utilisateur conçut comme idiot pendant la phase de conception d'un logiciel le restera tant que la définition du système ne change pas. Plus précisément, on pourrait considérer que ce paradigme de conception engage une double contrainte, c'est à dire une situation dans laquelle un individu soit se plier à deux obligations, chacune impliquant une interdiction de l'autre, selon la définition de G. Bateson (Bateson 1995). L'utilisateur final se trouve dans un système qui l'anticipe comme idiot et qui le somme parallèlement de se montrer « intelligent » et autonome dans la compréhension du système. N'étant pas un idiot systémique, l'utilisateur ne peut convenir substantiellement à la première définition, mais n'est pas pour autant en mesure de convenir à la seconde. Cette situation interprétée ainsi apparaît extrêmement pénible à vivre pour l'utilisateur qui quoi qu'il fasse est définit comme la cause des problèmes, qui seront forcément liés à un « mauvais usage ». Il faut toutefois nuancer le propos et signaler qu'au delà des modes de représentations, ce sont aussi les méthodologies de travail qui conditionnent la vision de l'utilisateur au sein du projet.

<!--
double contrainte de l'utilisateur ? Bateson : soit idiot (systémique), mais soit intelligent (usage) ?
-->


#### Enjeux du paradigme de conception

Au delà des modes de représentation et des outils et méthodes de conceptions, il faut s'intéresser plus spécifiquement aux théories qui soutiennent la conception graphique et le développement des artefacts numériques. Le concept de « do's and don'tst » traduit bien la pensée dogmatique qui peut résulter de l'apprentissage d'un savoir-faire. Car ce que l'on apprend à l'école, bien plus qu'un métier, c'est à appartenir au village des gens qui pratiquent ce même métier. Y. Citton définit le concept de la « co-attention présentielle » (Citton 2014) pour définir le processus réflexif par lequel l'attention des autres en présence influence notre propre attention. L'exemple le plus simple que l'on pourrait associé est l'expérience courante de porter instinctivement le regard vers une chose regardée par une autre personne dont on a conscience. Ce processus est aussi une affaire d'éducation, lorsque rassemblés en membres d'un même village, les individus apprennent à porter ensemble leur regard sur des choses définies. B. Bachimont pose comme base de la connaissance l'identification et la reproductibilité d'un geste, d'un concept, d'un fait de culture (Bachimont 2010). En ce sens, l'apprentissage du métier vise donc à apprendre à « faire attention à ... » et à reproduire les schèmes et à les individuer, c'est à dire à les définir comme une composante du soi, au sens de B. Stiegler (Individuation \| Ars Industrialis n.d). Ces schèmes sont à la fois à prendre au sens Kantien, dans le cadre de la perception et de la production du sens production du sens (Kant 1781), mais aussi au sens psychologique en cela qu'ils engagent ou prescrivent une action et un savoir conforté de manière empirique et itérative  (Piaget 2012). Les « do's and don'ts », traduit « à faire et à ne pas faire », sont des synthèses de cette construction de l'attention et de la connaissance durant la période d'enseignement. Ils sont exprimés sous forme de formules qui reprennent, d'une manière contextualisée, des dogmes et des façons de faire. On en trouve assez facilement en ligne avec des recherches du type « Comment faire un logo ? » ou « Comment faire du design graphique ? ». Voici une infographie produite par une agence de marketing digital, Vertical Measures :

![](./assets/img/infographics_do_dont.jpg) 
*Le document est structuré avec une colonne pour les exemples inacceptables du point de vue du dogme sur la gauche et les exemple acceptables sur la droite.*{: .legende}
{: .max-height}

La plupart des dogmes du designer graphique viennent de notions de hiérarchie de l'information et d'un soucis de lisibilité : ordonner la lecture des éléments par ordre d'importance, utiliser des niveaux de contrastes correct, etc. Certains aspects tiennent en effet du bon sens et valent en dehors du village des graphistes. Le problème vient malheureusement du manque de contexte, le design graphique est aussi une affaire de signification, et il va de soit que l'on peut rechercher à produire un sens qui sorte du cadre de la règle et du dogme. D. Carson est par exemple un graphiste qui se plaît à aller à l'encontre des règles typographique érigées par la discipline pour produire des décalages :

![](./assets/img/carson.jpg)
*D. Carson, (2014), Affiche du « International type symposium » de Bangkok.*{: .legende}
{: .max-height}

Le deuxième problème que peut soulever l'application de ces règles est de s'adresser en priorité aux membres de sont propre village, alors qu'il n'est pas nécessairement la cible du message ou de l'interface qui est conceptualisée. J'entends par là que le destinataire final de ce travail de conception n'est pas nécessairement éduqué à faire attention de la même manière qu'un designer graphique. Appliquer trop précautionneusement  toutes ces règles peut amener à créer un corpus de « lubies graphiques » que seuls d'autres graphistes seraient capable d'apprécier. Cette forme de sur-spécialisation formelle est applicable à n'importe quel village dans lequel est véhiculé un savoir faire. Le savoir faire est alors sur-réguler pour correspondre au savoir faire de ce village précis et pour tenter d'assurer sa reproductibilité à l'identique. P. Amiel rend bien compte de ce phénomène dans la réalisation simple d'un tarama, qui dans le village familial de l'auteur est un véritable account, un élément significatif du village, entouré d'allants-de-soi concernant la réalisation de la recette (Amiel 2010). Voilà aussi pourquoi certains dogmes graphiques ne nécessitent pas de justification, car leur justification est la plupart du temps communément admise dans le village des graphiste. Les « do's and don'ts» répondent donc systématiquement « Parce que. » à la question « Pourquoi ? ». Le travail de D. Carson peut donc être maintenant critiqué selon ce nouveau point de vue : son travail s'émancipe des codes graphiques mais pour mieux les imposer au destinataire, son travail pouvant être interprétée comme une « lubie graphique » globale visant à détourner les dogmes graphiques partagés dans son village, dont seul les autre graphistes peuvent capter la portée subversive. Mais encore une fois, les paradigmes et les dogmes de chaque village ne sont pas un mode d'analyse suffisant. Il faut pour cela s'intéresser au processus de travail, à la part d'attention accordée à la définition de  l'utilisateur et de son terrain en amont de la conception.




#### L'éthnométhodologie comme outils du dépassement

Alors que faire ? En sachant que « Celui qui veut agir et parler avec justesse finit par ne rien faire du tout. » (Nietzsche), que « Ne rien faire c'est déjà faire quelque chose. » et que « Qui veut faire quelque chose trouve un moyen. », mais aussi que « Le mieux est l'ennemi du bien. » (Montesquieu) ? Ce n'est certainement pas par les formules dogmatiques que nous allons réussir à transcender les prescriptions d'agir qui sont multiple dans un écosystème. Je crois que le mieux que nous puissions faire est de prendre du recul et à minima d'avoir conscience des structures dans lesquelles nous nous inscrivons. D'une manière plus pragmatique, j'aimerais proposer dans le cas présent les études de terrain comme un palliatif à l'arraisonnement de la dimension humaine par les artefacts techniques qui sont censés incarner la solution à nos problèmes. J'entends par là qu'il y a dans le cas du designer et du développeur un objectif commun : la résolution d'un problème, la réponse à un besoin. Ce besoin ou problème a une origine identifiable, et contrairement à ce l'on pense communément, le mandataire qui formule ce besoin n'est pas forcément le mieux placé pour le définir. Que ce soit en développement informatique ou en graphisme, le contexte initial du besoin et ses enjeux sont tronqués soit dans un brief, soit dans un cahier des charges. L'esprit averti vous dira que « S'il est tronqué, c'est qu'il est mal fait. », et donc que tout ce qui sera bâti à partir de celui-ci sera une perversion de la manifestation théorique pure du besoin. En ce sens tout mon argumentaire sur les modes de représentations et des paradigmes de conceptions s'en retrouverai caduc. Je contre argumenterais donc à ce sujet que le besoin formulé par le client sera forcément mal fait puisqu'il est formulé, la plupart du temps, par des représentants du personnels et pas par le personnel lui même ; que s'il est formulé par l'entreprise qui produit la solution, il sera dénaturé par une vision issue de village de l'entreprise et du village plus vaste du domaine d'exercice de l'entreprise. Alors qui doit formuler le besoin ? La réponse que je vais donner est simple : par tous ceux impliqués dans le projet, de la conception, à la production, à l'exploitation, à l'usage. Le besoin doit être formulé sous la forme d'une analyse croisée entre les salariés sur le terrain, leurs dirigeants censés rendre compte des enjeux et objectifs de l'entreprise, l'entreprise de production de la solution, représentée par tous les corps de métiers différents impliqués dans le projets. La question suivante est de se demander quelle méthode magique pourrait permettre de rendre compte de tout ceci sans pervertir le propos de chacun des acteurs. La réponse ne peut être formulé sans prôner une appartenance plus ou moins forte de ma part à une chapelle ou à une autre. La réponse est donc que je n'en sais rien, mais que je vais m'appuyer sur mon expérience personnel pour proposer des candidats à cet exercice.

D'une manière générale, l'ensemble des sciences humaines et sociales tentent de définir avec chacune un prisme différents les « réalités humaines ». Les sciences « dures » quand à elles tentent de donner des modes d'explication aux faits de la nature et à un ensemble d'autres faits logiques et formels. On trouve en seconde annexe du livre de P. Amiel un manifeste de Y. Lecerf sur la « science comme réseau » (Amiel 2010). Dans ce texte, la tentative est faite de démontrer que chaque science prise de manière unitaire, implique un rationalisme localisé, en opposition à un universel observable, autrement dit UNE réalité. Le propos consiste à dire qu'il n'existe pas physiquement d'observateur omniscient et objectif, et qu'il n'y a pas d'observation sans observateur. L'observateur est toujours le membre d'un village particulier, attribuant aux événement son mode d'explication singulier. Le fait expliqué est donc toujours singulier, jamais universel. Qu'il ne soit pas universel ne veut pas dire qu'il n'est pas partageable ou reproductible, mais simplement que sa reproductibilité dépendra, entre autre, de l'observateur. La méthode d'observation de terrain avec laquelle j'ai été familiarisé le plus tôt est l'ethnographie. L'ethnographie vise à documenter un terrain et ses occupants d'une manière descriptive. Les moyens utilisés pour documenter importe peut, il s'agit surtout d'une captation massive de données issues du terrain. Initialement, l'ethnographie est la phase qui précède l'ethnologie. Le premier étant donc la phase de documentation et le second la phase d'analyse et de création du concept de « l'éthnie » observée. Dans mon cas, cette documentation a servi en préalable à un travail de design global sur la médiathèque de Vienne en Rhône-alpe. La première phase a été de rencontrer le dirigeant de la médiathèque qui nous a présenté sa vision des enjeux de son institution. La seconde a été une captation libre sur le terrain constituée de prise de notes, de relevés, d'entretiens. Ce n'est qu'après avoir recueilli toute cette matière et avoir produit une analyse que le travail de conception a pu commencer. Une phase qu'il aurait été intéressante de mené est celle de l'analyse de notre propre terrain, en forme de micro agence de design constituée de trois élèves. Bien sur, on peut prendre pour acquis nos propres allants-de-soi et présupposés étant donné que nous ne pouvons objectivement les repousser. Néanmoins, il aurait été intéressant que chaque membre de l'équipe, avant même l'analyse du terrain, partage une vision de ses propres enjeux et attentes dans le projet. Cette vision aurait ensuite permis d'avoir un mode d'explication utile dans les prises de positions et dans l'argumentaire proposé par chacun dans le projet. Je ne connaissais pas à l'époque l'ethnométhodologie et il me semble que certaines notions auraient permis de dégager encore mieux les propriétés du terrain. L'indifférence éthnométhodologique par exemple vise à se conceptualiser soit même dans le village d'étude comme un observateur, un observateur bien inclut dans le terrain, qui modifie donc l'observation, tout en suspendant son jugement personnel, cette notion est théorisée par H. Garfinkel (Garfinkel 1984). Cette indifférence est une posture double théorique qui ne refuse pas la prise de position et le jugement de valeur, qui de toute façon émergent naturellement, mais qui oblige à indiquer chacune de ces prises de positions en tant que tel. L’enjeu est de permettre de suivre la construction du sens dans l'analyse et dans le processus d'observation, pour enrichir d'autant plus le matériau documenté. Si ethnométhodologie n'incarne pas la figure d'émancipation de toutes nos prescriptions, elle tente de mettre en place une documentation la plus objective et complète possible du terrain.

Fonctionner ainsi, c'est s'assurer de dresser une analyse, assez fine à mon sens, de tous les enjeux qui traversent le projet pour un maximum d'acteurs. Le matériau final doit ensuite être communiquer et mis en forme pour le rendre manifeste comme idéologie et besoin dans le projet. Lors de ce projet à la médiathèque, nous avons pour ce faire utilisé des *personae*, à savoir des représentation d'individus, fictifs pour le projet dont nous parlons, censés représenter le panel représentatif des destinataires du projet. Pour ce projet les *personae* étaient fictifs, mais il est évident qu'il vaut mieux les baser sur les destinataires réels du projet. L'idée de « panel représentatif » peut aussi faire grincer des dents car pour être acceptable il faudrait qu'il soit constitué d'une manière scientifique, sur la base d'un échantillonnage fin sur le terrain et sur une juste proportion des types. On ne peut éviter à mon sens d'établir des types dans ce panel, l'un des enjeux du mandataire étant, la plupart du temps, de toucher l'ensemble des membres du village. La typification des *personae* est alors une manière de confronter le projet en évolution à chaque sensibilité en tentant de se projeter dans notre vision de chaque type. Une fois ces *personae* définis, nous avions dressée une fiche succinte de chacun de sorte qu'ils soient toujours sous notre regard pendant la conception du projet. Rendre ainsi manifeste le plus d'éléments possible du terrain permet d'effectuer une confrontation réflexive, durant la conception, entre les concepts et les idées formulées, et les enjeux multiples dégagés par l'analyse. Bien sur, cette phase de conception en confrontation nécessite souvent un retour physique au terrain, avec des phases de tests d'usage et de discussion avec les membres du village, pour vérifier que le projet suis toujours sa condition d'être. B. Bachimont dans la toute fin de son œuvre (Bachimont 2010) propose comme figure du dépassement aux prescriptions des dispositifs et artefacts numériques la figure de « l'ingénieur-artiste », garant de la cohérence des dispositifs. Trois types de cohérence s'entrecroisent dans l'ouvrage de B. Bachimont :

+ La cohérence interne qui correspond au savoir techno scientifique qui sont la condition d'existence de l'artefact,
+ La cohérence concrète qui constituée de la condition matérielle de l'artefact et sa réalisation,
+ La cohérence externe qui incarne l'argumentaire et la légitimité sociale de l'artefact.

J'ai posé moi même l'an dernier, dans un autre mémoire, la figure du designer comme garant de ces cohérence. Je pense aujourd'hui avec du recul que cette vision était teinté d'une trop grande influence du village des designer. La raison pour laquelle j'avais émis cette proposition est que le design use de plus en plus de méthodes issus des sciences sociales et que ce sont ces méthodes d'analyse que je plébiscite en ce moment même en amont des projets. Comme vous l'avez compris j'avais découvert et expérimenté ces méthodes la même année. Ainsi, il m'apparaissait évident que le designer était celui qui réaffirmait la nécessaire considération du terrain. Je dirait aujourd’hui que le design a cela d’intéressant qu'il est une éponge aux autres domaines scientifiques et que le design porte substantiellement la capacité de faire des emprunts ingénieux aux autres disciplines et sciences. Pour rétablir aujourd'hui l’équilibre, je dirais que ce n'est pas parce que le design, entre autres disciplines, rend concret l'application de ces théories dans la conception qu'il est dépositaire du savoir et des méthodes qu'il emprunte. Je dirais donc que cet ingénieur artiste est une pure vision de l'esprit, et qu'il se réfère simplement aux figures de dépassement proposées par B. Bachimont :

+ La cohérence interne : le savant,
+ La cohérence concrète : l'artiste,
+ La cohérence externe : le politique.

En proposant l'ingénieur artiste, B. Bachimont implique peut être que le dispositif, concrétisé par l'ingénieur, nécessite que l'ingénieur en question dépasse le champs du concret pour investir celui de l'art, s'émancipant ainsi d'un régime purement technique. À noter que B. Bachimont est, entre autres choses, docteur en informatique et ingénieur civil, ce qui peut éventuellement prescrire un choix légitimer par sa propre expérience. Pour ma part je dirais donc qu'aucun être ne peut être dépositaire de la cohérence de l'artefact en devenir et qu'il s'agit d'établir dans les équipe de conception un protocole qui permet de le garantir, mais surtout d'établir une posture critique de chacun sur cette question.
<!--
Ethnographie = documentation des pratique suivie d'une étude Ethnologique = Tirer des traits généraux (induction)
Ethnométhodologie = captation des accounts d'un village
-->


### Discours marchand de l'innovation, du nouveau, de la complexité

L'argumentaire développer précédemment préconise donc un retour au terrain pour déterminer la direction d'un projet et en assurer la cohérence continuellement. Cet argumentaire s'applique localement au projet et peut éventuellement permettre de s'émanciper d'un contexte plus globale que serait le contexte économique de l'entreprise. Encore faut-il cerner ce qui constitue ce contexte plus global et quels sont les discours qui le traverse. Pour ce faire, décrypter les arguments de vente autour des « nouvelles » technologie est un mode d'explication intéressant, tout comme évaluer le positionnement de divers acteurs du marchés. Un point de vue pragmatique sur les cet argumentaire semble aussi être un bon moyen de comprendre si la manière dont nous qualifions les artefacts numériques s'accorde à des faits ou à des fantasmes.
{: .abstract}



#### Ambivalence accessibilité et ouverture

La concurrence entre Apple et Microsoft est une bataille qui s'est jouée et se joue toujours dans nos foyers. Le fait de se revendiquer « Mac » ou « PC » implique plus que la propriété d'un bien de consommation mais sert souvent d'argument d'autorité dans les conversations entre amis. Certains secteurs professionnels plébiscite l'un au détriment de l'autre et la possession d'un appareil Apple pour le travail constitue même parfois un symbole d'appartenance à un village particulier. Les designers par exemple s'identifie assez à la firme à la pomme, d'une part au travers de ses produits mais aussi au travers des figures importantes de la firme. Dans l'annexe de ce mémoire concernant la vision du statut de designer et de développer par la sous-culture web[^annexe_meme] vous remarquerez que l'illustration mise en avant pour le designer se définissant lui même est celle de S. Jobs, la figure emblématique de la firme à qui l'on attribut les plus grands succès. Si la prise de position implique autant socialement, c'est qu'il s'agit aussi d'une bataille d'idéologie, de communication et de positionnement marketing.

Lorsque sort le premier Macintosh en 1984, Apple se sert de l'année symbolique, en référence au livre de G. Orwell « 1984 », sorti en 1949, pour développer une rhétorique forte et planter les bases d'une idéologie qui continu encore aujourd'hui d'être une caractéristique de la firme. La vidéo publicitaire associée à la sortie du produit, diffusée le 22 janvier de cette même année, durant le Super Bowl, montre une assemblée au crâne rasée, assistant à l'élocution projeté, à la manière d'un cinéma, d'un dirigeant en gros plan qui incarne le Big Brother du livre. Toute l'esthétique du clip reprend l'univers du livre et du film éponyme sorti la même année. À la moitié du clip apparaît une jeune athlète blonde en tenu de compétitrice. La jeune femme est poursuivie par des hommes que l'on peut interpréter comme étant une milice qui soutient le gouvernement. Au trois quart du clip, la jeune femme parvient au niveau de la projection et jette, dans un geste évoquant celui du lancé de marteau olympique, un marteau qui vient percuter l'écran. S'ensuit une explosion de lumière, qui stupéfait l'assistance dégarnie, puis le message publicitaire suivant apparaît :

> « Le 24 janvier, Apple vous présentera le Macintosh. Et vous verrez alors pourquoi 1984 ne sera pas « 1984 ». » (Publicité Apple « 1984 » )

Le clip devient mythique et est aujourd'hui cité en exemple dans les écoles de design et communication. D'abord parce que la cohérence globale de la date et du propos est parfaite, mais surtout parce qu'Apple avec ce clip livre un message on ne peut plus clair. Big Brother est la personnification d'IBM, concurrent de Apple, qui commercialise depuis 1982 le premier PC (Personal Computer), le « IBM 5150 ». Le point notable de ce modèle d'IBM est qu'il ne charge son système d'exploitation au démarrage, mais seulement une console d'interprétation d'un langage formel : le BASIC. Le système d'exploitation est fournis sur disquette, comme l'ensemble des logiciels, il s'agit de la première version de Windows alors baptisé « MS-DOS » et il faut exécuter une commande en BASIC pour le lancer. À côté du PC d'IBM, le Macintosh d'Apple utilise une interface graphique et une souris dans un environnement multitâche tel que nous le connaissons aujourd'hui. Le message est donc que le Macintosh vient délivrer l'utilisateur d'un système trop complexe pour offrir un contrôle spatialisé du système. La lutte idéologique qui s'est qui s'est résolue à ce moment là est perceptible dans le film biographique « Steve Jobs » (Steve Jobs 2015), qui donne une bonne idée des tensions idéologiques et des enjeux qui existait à la conception de la première machine commercialisée d'Apple, et aussi son premier succès : le « Apple II ». Le commentaire qui va suivre concerne la description de la scène du film. Il faut donc comprendre derrière les nom S. Jobs et S. Wozniak que je vais parler des personnages du film, qui reste une fiction. Le film montre une scène dans laquelle S. Jobs et S. Wozniak dans un garage, argumentant en faveur d'un système ouvert (S. Wozniak), ou fermé (S. Jobs). Le film dépeint le second comme un être centré sur lui même et dans cette scène, S. Wozniak assène une réplique lourde de sens à son collègue et ami :

> « Les ordinateurs ne devraient pas souffrirent de défauts humains. Je ne vais pas concevoir celui-ci avec les tiens. » (Traduction)(Steve Jobs 2015)

Tout au long du film, S. Jobs apparaît comme un homme qui doit avoir un contrôle permanent sur les choses, un contrôle qu'il veut intégrer au système. Ce contrôle en système fermé implique que les logiciels existants ne seraient pas compatible avec ce nouveau système et que l'extension des composants seraient limités. S. Wozniak s'y oppose en prônant la création d'un système ouvert, disposant de ports d'extension multiple pour adapter l'ordinateur à différents besoins. En cherchant sur internet on peut encore trouver quelques descriptifs de ces cartes d'extensions. La plupart permettent d'interfacer des périphériques extérieur (imprimantes, modem, etc.), mais on trouve aussi des extensions étonnantes. Voici par exemple un extrait d'une publicité journalistique pour la carte « Wildcard II » :

> « Wildcard. Faites des copies de logiciels protégés rapidement et facilement, d'une simple pression de bouton. Maintenant ! Trois modèles de la première carte de copie du marché. L'une d'elle est pour vous. » (Inc, 1984)

InfoWorld engageait des équipes constituées essentiellement de journalistes experts en informatiques. Sa parution a commencé en 1978 et a transité vers un site web en 2007. Le site est toujours en activité et propose aujourd'hui une variété plus importante d'actualités qui cible les consultants en technologies et les administrateurs de parc informatique. Pour en revenir à la carte « Wildcard II », on s'imagine assez facilement le problème que peut poser une fonctionnalité comme celle-ci dans notre contexte économique actuel. Bien loin d'une logique propriétaire, la carte permet de copier un programme, focalisant son argument de vente sur la copie de logiciels protégés. Dans le film comme dans la réalité, rien ne dit en revanche si S. Jobs était opposé à de telles pratiquent mais son idéologie d'un système d'exploitation dans le film n'est pas celle d'un système ouvert et personnalisable. Le Macintosh qui sort en 1984 est un système fermé. La réside un paradoxe intéressant avec le clip publicitaire « 1984 » : Apple délivre l'utilisateur en offrant un puissant système d'exploitation qui permet un contrôle graphique, mais inclut dors et déjà toutes les métaphores de bureaux, d'icônes, le principe du WYSIWYG, qui démocratisera ce paradigme auprès du public, créera une habitude d'usage et annoncera le monopole de ce paradigme pour tous les utilisateurs de PC. Aussi, la logique du système d'exploitation fermé rentre en conflit avec la rhétorique que propose la publicité étant donné qu'Apple contrôle les logiciels, la conséquence pour l'utilisateur est un panel de logiciels plus restreints, le Macintosh ne représentant pas une part de marché intéressante pour le développement logiciel, là ou les « Compatible PC », à savoir les ordinateurs compatibles avec le PC d'IBM représentent une cible beaucoup plus large du fait de la compatibilité.

En août 1998 est commercialisé l'iMac G3, l'un des éléments frappants de ce nouveau produit est sa transparence, sur laquelle S. Jobs fera lors de la séance d'ouverture officielle. Cette transparence n'est pas anodine et on peut tenter de la mettre en perspective avec le fait que le système d'exploitation Mac OS, depuis le Macintosh, est un système fermé. La transparence de l'iMac donne une impression d'ouverture et d'accessibilité des composants, que l'on peut voir au travers de la coque.


#### Critique du « nouveau » et de l'innovation

Le principal argument commercial autour des nouvelles technologies est souvent celui que celles-ci vont « nous changer la vie ». Les expressions telles que « révolution numérique » sont reprises comme des impératifs à nous équiper de toute sorte d'appareils sans avoir même penser au préalable ce qu'ils allaient changer concrètement dans nos vies et dans nos usages; Y. Jeanneret (Jeanneret 2007) met d’ailleurs en exergue ce fait dans son ouvrage au travers du questionnement du mot « nouveauté ». Il identifie dans l'expression « nouvelles technologies », trois aspects :
+ la nouveauté technique, dans laquelle on retrouve les fonctionnalités du dispositif et le fait qu'il soit « nouveau », au sens matériel de « inexistant auparavant »,
+ la nouveauté sociale, c'est à dire les usages qui accompagnent se nouvel artefact,
+ la nouveauté médiatique, celle des annonces et du débat social, de la médiatisation du dispositif et de sa promotion

Le propos d'Y. Jeanneret consiste à dire que l'expression « nouvelles technologies », ces trois nouveautés distinctes sont intriquées, de sorte que tout discours dans lequel est employé l'expression peut se servir de l'un ou de l'autre des aspects dans sa rhétorique. Elle force à admettre l'idée même que le dispositif proposé est nouveau car la nouveauté technique et matérielle d'un dispositif est indéniable, mais par la même occasion, elle fait admettre que le dispositif l'est sur les deux autres plans. On pourrait même ajouter une quatrième « nouveauté » à cette liste en la liant au principe de cohérence de B. Bachimont développé plus tôt (Bachimont 2010). Cette définition diffère quelque que peut et nous permet d'ajouter à cette cohérence un quatrième pendant : une cohérence que l'on pourrait nommer de « pragmatique » et qui corresponds à la « nouveauté sociale » de Y. Jeanneret. Dans l'autre sens, l'ajout que pourrait apporter la cohérence de B. Bachimont à la vision de la nouveauté de Y. Jeanneret serait justement la « nouveauté technologique », qui consiste en tout le savoir techno-scientifique qui rend possible la réalisation concrète, « technique » et non plus « technologique », de l'artefact. Le dispositif rend en effet manifeste les lois scientifiques qui sont sa condition d'existence et qui le préfigurent, encourageant l'engouement technologique et un régime attentionnel dans lequel nous « attendons la nouveauté » en portant attention aux avancées scientifiques qui annoncent l'arrivée dans notre quotidien d'une quantité de « nouvelles choses ». 

Le mot « innovation » est d'ailleurs en ce moment en vogue et souffre des mêmes problèmes que le mot « nouveau ». On ne sais pas de quelle innovation on parle et les « startup innovantes » naissent et meurent aussi vite que les idées. Un vent d'optimisme pour l'entrepreneuriat secoue l'économie, avec une flopée de prix différents destinés à récompenser les meilleures idées. Le problème du mot innovation est malheureusement plus vaste. Dire qu'une startup est innovante, c'est anticiper la réalisation de quelque chose de nouveau qui sera une nouveauté sociale ou technique à minima. Là où le bas blesse, c'est que les startup « innovantes » sont simplement souvent des startups avec de « bonnes idées » et que de l'idée à l'innovation il existe un décalage qui n'est pas dépendant que de la startup mais surtout de la manière dont l'idée matérialisée, concrétisée, sera reçue socialement.

Un autre phénomène pointé du doigt par Y. Jeanneret, c'est l'assimilation des propriétés du support au document lui même. 


#### L'élixir ou le poison, discours technologique thaumaturgique

La notion de « nouveau » développe donc l'argumentaire d'un changement notable qu'il faut, si possible, ne pas louper. Un autre discours entourant les dispositifs techniques consiste à affirmer leur bienfait nécessaire en tant qu'objets issus d'un progrès technologie. Or, il s'agit encore d'un amalgame entre les différents prismes qui permettent de considérer le progrès. Un progrès technique n'est pas nécessairement un progrès social, la vérité étant simplement la plupart du temps que le progrès technique crée une alternative, une manière de faire autrement, dont les bienfaits dépendent du point de vu et de l'observateur. L'ambivalence de la technique « pour le pire ou pour le meilleur » est développée dans la notion de *pharmakon* dont on peut remonter l'origine à la « Pharmacie de Platon » de J. Derrida (Platon, Derrida et Brisson 2006), qui propose une analyse de l'emploi du mot *pharmakon* par Platon et établissant l'ambivalence entre remède et poison. B. Stiegler reprend ce terme pour définir une caractéristique substantielle de la technique qui serait de nature *pharmacologique*, à la fois poison et remède, dans la mesure où c'est l'usage et la valeur sociale qui condamne ou prescrit cet usage qui déterminent le bien ou le mal (Pharmakon | Ars Industrialis, n.d). 

Le discours marchand qui entoure un dispositif technique a sa sorti est nécessairement mélioratif, puisqu'il s'agit de vendre un produit. On imagine en effet mal un vendeur de téléphone portable argumenter que « Vous ne verrez plus vos proche car vous aurez le nez dans votre écran. Toujours disponible, vous pourrez vous isoler instantanément de votre environnement direct à n'importe quel moment. », bien que cet argument puisse tout de même séduire suivant le point de vue. Le vendeur aura au contraire le comportement inverse : « Vous pourrez garder toujours vos proches près de vous et n'aurez plus à vous inquiéter de leur situation : demandez leur simplement où ils sont et ce qu'ils font quand vous le désirez. ». Si le vendeur est particulièrement investi, il ira même par vous dire qu'avec un tel téléphone « Vous réfléchirez plus efficacement vos projets. ». Ce dernier argument est ce que Y. Jeanneret appelle le « discours thaumaturgique », qui relève du miracle, le pendant bénéfique du *pharmakon*. Il est largement distillé dans la publicité, de manière plus où moins implicite. Le slogan de Microsoft par exemple (« Be what's next », *Prenez de l'avance*), est une projection conceptuelle totale, sans lien véritable avec les produits, des propriétés supposément bénéfiques de l'informatique. Le mélange du social, du technique, du technologique est encore une fois présent : *Prenez de l'avance* (sur qui ? sur quoi ?), avec toujours cet impératif de ne pas louper ce que serait nouveau. Une propriété ciblée ici est l'efficacité que promet l'imaginaire autour des artefacts numériques, faits de calculs savants, permettant de manipuler des documents, en oppositions à la lourdeur matérielle du papier. L'autre propriété ciblée est la promesse d'une nouveauté plus nouvelle que toutes les autres, sous-entendu « Prenez de l'avance technologique ».

L'idée que l'artefact est plus qu'une « proposition communicationnelle », c'est à dire un ensemble de modalités d'interactions qui structure ma manipulation, la création et la diffusion d'information et ne portant pas d'ontologie avant son usage (Y. Jeanneret 2007), est présente dès le début des recherches en interaction Homme-machine. En 1962, D. Engelbart publie un essai intitulé « Augmenting Human Intellect » (Augmenter l'intelligence humaine)(Engelbart 1962), qui est déjà une expression thaumaturgique. L'essai traite du traitement de problèmes complexes, outillé par l'informatique. Les première lignes confortent dans l'idée qu'il s'agit en fait d'apporter de nouvelles représentations des problèmes complexes grâce à l'outil informatique :

> « Par « augmenter l'intellect humain » nous voulons dire augmenter sa capacité à approcher un problème complexe, pour le comprendre en cohérence avec ses besoins spécifiques et de ce fait déduire des solutions aux problèmes. » (Engelbart 1962)

Il s'agit plus d'outiller la perception plutôt que d'outiller l'intelligence, celle-ci restant inchangée mais les représentations des problèmes qu'on lui soumet étant changés par les moyens informatiques. 

Le discours marchand entretient donc l'illusion d'un progrès nécessairement bénéfique par des amalgames multiples entre les technologies et les techniques qui en résulte, la notion de progrès et de nouveauté et finalement d'efficacité. Une efficacité qu'il faut traquer jusque dans les interfaces, dont nous avons vu qu'elles misent sur des principes qui tentent de limiter les apprentissage pour aller directement à un usage intuitif des artefacts numériques. On peut alors reposer la question de ces principes mais sous un angles économique pour déterminer le bien fondé de certaines récurrences formelles dans les interfaces que l'on manipule.


### Recherche d'efficacité : écologie de l'attention et design d'interface

Les interfaces ne sont pas nécessairemment le fruit d'une analogie et certains motifs récurrents nous permettent de le appréhender sans avoir à tout réapprendre. Ces motifs trouvent leur cohérence dans les interfaces numériques et n'ont pas nécessairemment de pendant matériel. Ils ont émérgé avec les besoins de l'affichage d'informations à l'écran, en cohérence avec un environnement WIMP. La première interface grand public du Macintosh intègre déjà tout ces éléments et le paradigme d'interface des ordinateurs personnels n'ont pas changés. La question est donc de savoir quelle est la raison d'être de ces motifs dans l'interface du système d'exploitation et de savoir si c'est conditions d'être sont immuables.
{: .abstract}




#### Formes récurrentes des interfaces

L'acronyme WIMP signifie *Windows, Icons, Menus and Pointing control*. Nous sommes familiers de chacun de ces éléments, à tel point qu'ils sont légitimés en soit. En étant la condition d'action sur la matière numérique, il imposent leurs formes normalisés comme naturelles. À tel point qu'il m'est déjà arriver de chercher la présence de la barre de menu par exemple dans un logiciel qui n'en contenait pas. Y. Citton dans sa théorie de l'attention dirait qu'il s'agit d'un envoûtement formel, qui pousse à porter notre attentions sur des éléments que nous savons déjà reconnaître (Citton 2014). Si nous sommes habitués à trouver une fonction à un endroit précis, il est donc logique de chercher la forme qui la rend accessible lorsque nous recherchons cette même fonction. L'absence de la barre de menu (Fichier, Édition, Affichage, etc.) donne donc l'impression que les fonctions associés ne sont même pas présentent dans le programme. 

La force de ce paradigme est que chacun de ces concept en légitime un autre. Le concept de fenêtre par exemple légitime la présence d'un menu standardisé disponible dans chaque fenêtre. La barre menu est en effet le seul élément dans une application qui fasse référence à l'application en tant que fenêtre et en tant que composant du système d'exploitation. La barre de menu donne par exemple accès à des fonctions d'affichage qui permette de changer le comportement de la fenêtre dans l'espace de l'écran. Le menu fichier permet d'accéder aux fonction de gestion de fichier du système, un composant indépendant de l'application. Le fait que l'application s’exécute dans une fenêtre légitime donc la nécessite d'une chose qui permette de la lier au système. Si l'application était constitués d'instruments comme le théorise M. Beaudouin-Lafon (Beaudouin-Lafon 2000), elle donnerait alors simplement accès à des moyens de manipuler la matière numérique qui serait indépendants de la notion d'espace d'application et qui pourrait s'exprimer sur toute la surface de l'écran. Mais ce n'est pas le cas, étant donné que nos documents sont en fait des espaces applicatifs qui contiennent des outils précis à l'espace de la fenêtre de l'application.

Le fait que l'interaction soit basé sur un système de pointage légitime les icônes, les liens, toutes formes de boutons. Le dispositif de pointage implique en effet une cible spatialisé pour pouvoir être utilisé, ce qui ne serait pas le cas si l'ordinateur utilisait une interface de contrôle langagière, dans laquelle les cibles ne seraient pas icônes spatialisés mais des noms, des expressions ou bien des sons. Le contrôle pourrait aussi être gestuel, comme on peut le voir avec des dispositifs comme le capteur SOLI de Google qui permet de détecter des mouvements de la mains très fins, dont chacun pourrait être associé à une commande précise, au delà du simple fait de sélectionner des cibles interactives bien sur. 

Le modèle WIMP est en fait un modèle très cohérent basé sur l'interdépendance des éléments. Pour ouvrir l'imaginaire et permettre d'inventer d'autres modèles, je propose une réduction conceptuelle de l'acronyme WIMP. L'acronyme EIAC signifie Espaces, Identifiants, Actions, Contrôleur. L'idée en formulant les choses ainsi est de réduire chaque éléments du WIMP à ce qu'il constitue à minima dans le système :
+ Les fenètres (W) sont donc des Espaces dédiés dans lesquels s'organise une activité,
+ Les icones (I) sont en fait des Identifiants qui permettent de naviguer dans l'information, de lancer des fonctionnalités, etc.,
+ Les menus (M) sont des actions décontextualisés utilisable dans tous les espaces,
+ Le système de pointage (P) est en fait un contrôleur quel qu'il soit, qui fait interface entre une interaction matérielle et le système.

Cette approche permet de mener une analyse morphologique qui décuple les potentiels et permet d'imaginer d'autre systèmes d'exploitation. Les déclinaisons qui en découlent gardent substantiellement la cohérence du WIMP car elles sont des instances d'un principe généralisé, réduit, à partir du WIMP. Les appareils tactiles engagent des transformations de l'interface intéressant et les contraintes techniques forcent à penser les choses différemment, mais dans l'ensemble, la logique de pointage et encore au centre du contrôle. Récemment, l'usage d'une interface plus langagière qui consiste à taper ce que l'on cherche s'est démarquée. Sans aller jusqu'au traitement naturel du Siri de Apple, les logiciels et systèmes d'exploitation, conscient que nos applications sont de plus en plus nombreuses, donne accès à un mode de recherche par nom. Certains logiciels permettent aussi cette approche en permettant une recherche dans l'ensemble des fonctions et paramètres mis à disposition dans l'interface. L’intérêt d'une telle fonctionnalité est d'éviter l'empilement de fonctionnaliter et de proposer un moyen rapide d'accéder à une chose dont on connaît l'identifiant. Mais tout comme un site web ne saurait régler des problème de hiérarchie d'information et de typologie par une barre de recherche, un logiciel ne saurait clarifier son usage par la mise en place d'un champs de recherche. C'est pourquoi une autre solution est encore à imaginer.




#### Pour un éclatement de l'interface : contextualisation de l'action et substrat

La contextualisation de l'action et la personnalisation semble être de bons moyens d'éviter la surabondance d'affichage de fonctions. Dans Office Word par exemple, l'ensemble des options de formatage du texte sont disponible en permanence pour permettre de sélectionner un changement de corps ou de graisse avant de taper. Une première idée voudrait que le formatage de texte soit contextualisé au moment de la frappe selon un scénario de ce type :

+ Je commence à taper une phrase, à côté du curseur de texte se crée une icône de formatage qui reste un moment à la fin de ma frappe et disparaît si l'on ne clique pas dessus,
	+ ![](./assets/img/context_1.png) 
+ Au clic sur cette icône, un menu circulaire me propose les options de formatage classique (typographie, graisse, couleur, interlettrage, interlignage, etc.) typologie par thématique,
 	+ ![](./assets/img/context_2.png) 
+ Une fois le formatage sélectionner, le logiciel actualise l'affichage pour être conforme à mon choix, en transformant tout le texte issu du dernier enchaînement de frappe réalisé.
	+ ![](./assets/img/context_3.png) 

Les fonctionnalités contextualisées de cette manière permettent d'alléger l'interface des informations qui ne sont pas nécessairement à l'action. De plus, ce fonctionnement serait unifié à l'ensemble des logiciels dans lesquels l'édition de texte est possible pour ancrer un référentiel pérenne dans tout le système. La contextualisation engage la création d'une typologie de classification des éléments graphiques à l'écran. Les fichiers sont déjà des entités reconnaissables par une typologies, mais cela n'affecte que les espace-applications dans lesquels ont peut les éditer. Créer une typologie qui réifie la matière numérique à l'écran permet de contextualiser des actions quel que soit l'espace d'édition. Cette vision amène aussi une interopérabilité sur la matière numérique. M. Beaudouin-Lafon développe le concept de substrat pour qualifier la matière numérique immatérielle, qui peut être éditée dans différents contextes (Klokmose, Eagan, Baader, Mackay et Beaudouin-Lafon 2015). Une des propriété du numérique étant que l'inscription est immatérielle, en la considérant comme donnée non incarnée, on peut d’ailleurs imaginer décorréler le contenu de sa présentation graphique. Mais cela implique un positionnement de fond sur la question de ce qui est signifiant dans la matière inscrite à l'écran, la mise en forme du texte faisant partie du processus d'extraction du sens depuis la matière morte qu'est l'écrit. La théorie du substrat veut que le contenu transcende les espaces, ce qui n'est pas du tout le cas dans nos ordinateurs, qui spatialisent les fichiers et les dossiers dans un modèle imbriqué proche de la métaphore matérielle du bureau et de ses tiroirs ou chemises de rangement. Néo est par exemple un concept de système d'exploitation qui permet de classifier les fichier par des tags plutôt que par des dossiers. Ce système permet alors d'éviter le phénomène de « poupées russes » du modèle arborescent et permet de qualifier avec plusieurs termes et à niveau égal un même document tandis que des dossiers impliquent des catégorie et sous-catégories de classification.

En définitif, contextualiser les élément d'interface, c'est réifier la matière numérique en objets qui transcendent les espaces d'édition. C'est faire des pixels un substrat logique sur lequel le système vient greffer localement les fonctions sur ce substrat. Cette greffe locale ne constitue pas un environnement d’édition restreint mais une somme d'outils monopolisés en même temps, de manière cohérente avec la le type d'objet à éditer. En somme, une boîte à outil numérique convoquée par dessus le substrat pour le rendre manipulable.
    
<!--
liien seuil et bdd
TODO: La grammatisation c'est la reconnaissance des affordance computées
TODO: Aurore: facilité à changer d'environnement
-->


#### Création des méthodes par l'utilisateur : proposition de dépassement

Les fonctionnalités d'un logiciels sont finies : à l’exécution du programme, les fonctionnalités préprogrammées sont chargée et l'interface nous en donne l'accès. Éventuellement, certains logiciels permettront d'articuler ces fonctionnalités via un système de *script* ou *macro*. La finitude de ce système est l'automatisation de tâche plus ou moins complexe. Les scripts sont même dans certains cas des possibilité d'extension du logiciel, au sens où celui-ci permet parfois de créer des éléments d'interface qui permettent de paramétrer le script et interagir avec.

![](./assets/img/scripts_dialog.jpg)
*Exemple d'une boîte de script dans Adobe After effect*
{: .legende}

L'édition de script est un élément essentiel pour qui veut adapter les fonctionnalité à des contextes précis différents. Étant donné que les logiciels tendent à être des omnilogiciels, c'est à dire convenir à l'édition global d'un type de média dans son ensemble, il est parfois nécessaire de les étendre pour les rendre spécifique à l'édition d'une typologie de documents en particulier. Un document administratif par exemple ne partage pas la même disposition qu'une affiche ou un lettre. Les catégories de documents sont identifiés en fonction de la disposition, dans l'espace documentaire, au sens de Y. Jeanneret, à savoir dans tout l'espace de présentation matériel du document (Jeanneret 2007), des éléments de contenu. Il arrive aussi souvent de créer des modèle de documents qui servent à les identifier en tant que tout cohérent. Dans ces circonstance il peut être intéressant de définir des scripts qui systématisent certaines tâches de présentation. À mon sens, tous les programmes devraient inclurent des scripts car ils sont un moyen pour l'utilisateur de faire convenir le logiciel à son besoin.

Seulement, écrire des scripts dans un langage formel n'est pas forcement à la portée de tout le monde. Pour palier à ce problème, on peut prendre exemple sur Tynker. Cet environnement de développement, plutôt destiné aux enfants, permet de leur apprendre la programmation en manipulant des briques logiques numériques.

![](./assets/img/tynker.png) 
*Un exemple de programmation basique sur Tynker*
{: .legende}

Voici à quoi pourrait ressembler un script dans le cas de notre traitement de texte précédent : `mettre en gras`{: .toolFunction} -> `souligner`{: .toolFunction} -> `augmenter le corps`{: .toolFunction} `de`{: .toolFunction} `3`{: .variable}.

![](./assets/img/context_0.png) 

*Exécution du script.*

![](./assets/img/context_3.png) 

Les briques logiques peuvent quasiment être lues en langage naturelles et permettent d'accéder aux fonctionnalités de l'application pour en tirer partie au mieux. D. Genthner et J. Nielsen déplorent d'ailleurs dans leur article (Gentner & Nielsen, 1996) le manque d'efficacité des interfaces basées sur la « manipulation direct », autrement sur le fait de cliquer sur chacune des fonctions à exécuter l'une après l'autre. Ce que l'article pointe du doigt, c'est d'une part la répétitivité de ce processus et le fait que l'utilisateur ne fait que « déclencher » les fonctions et les paramétrer. En ce sens, il se rapproche d'un opérateur. Aussi, l'article met en lumière que la nature calculatoire du système peut être bien plus précis que l’œil et la main, et être dynamique. Plutôt que de placer des élément au centre d'un dessin à la main, et devoir replacer ce même élément dès lors que la taille de l'image change, l'utilisateur devrait pouvoir renseigner l'ordinateur sur un « comportement » que cet élément doit maintenir, de sorte qu'il délègue à la machine le soin de recalculer sa position.

Les utilisateurs auraient énormément à gagner à pouvoir systématiquement « programmer » les choses pour pouvoir se décharger des tâches de manipulations spatiales de base et les traitements en séries. Mais qu'en pense l'utilisateur en question ? Car il est plutôt évident pour quelqu'un de familier avec l'informatique, qui aura l'habitude de détourner, adapter, programmer ses outils de prôner une solution qu'il maîtrise et dont il connaît la faisabilité et le potentiel. Mais qu'en est-il de l'utilisateur et comment se considère-t-il au sein du système ?




